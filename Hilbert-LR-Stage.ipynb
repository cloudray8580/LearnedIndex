{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                x        y       x_       y_        hilbert    order  \\\n",
      "0        15.24060  32.4770  2104812  2124770      387885300        0   \n",
      "1        15.55460  32.5908  2111092  2125908      437528096        1   \n",
      "2        15.60510  32.6348  2112102  2126348      451057380        2   \n",
      "3        15.59940  32.5877  2111988  2125877      451672609        3   \n",
      "4        15.60170  32.5865  2112034  2125865      451676231        4   \n",
      "5        15.60190  32.5874  2112038  2125874      451677074        5   \n",
      "6        15.61570  32.6319  2112314  2126319      451828539        6   \n",
      "7        15.65180  32.5395  2113036  2125395      453161461        7   \n",
      "8        15.63280  32.5325  2112656  2125325      453428057        8   \n",
      "9        15.64080  32.5314  2112816  2125314      453447428        9   \n",
      "10       15.64590  32.5313  2112918  2125313      453503767       10   \n",
      "11       15.64210  32.4882  2112842  2124882      453714402       11   \n",
      "12       15.64080  32.4881  2112816  2124881      453719209       12   \n",
      "13       15.63880  32.4877  2112776  2124877      453720601       13   \n",
      "14       15.64420  32.4944  2112884  2124944      453738000       14   \n",
      "15       15.66340  32.5070  2113268  2125070      453902180       15   \n",
      "16       15.64980  32.3915  2112996  2123915      454639221       16   \n",
      "17       15.64490  32.4704  2112898  2124704      454984708       17   \n",
      "18       15.61290  32.4547  2112258  2124547      455125665       18   \n",
      "19       15.61830  32.4712  2112366  2124712      455137468       19   \n",
      "20       15.58600  32.5286  2111720  2125286      456239228       20   \n",
      "21       15.58610  32.5310  2111722  2125310      456239512       21   \n",
      "22       15.58690  32.5263  2111738  2125263      456241049       22   \n",
      "23       15.59900  32.5239  2111980  2125239      456307567       23   \n",
      "24       15.60300  32.4970  2112060  2124970      456411550       24   \n",
      "25       15.60730  32.4999  2112146  2124999      456429849       25   \n",
      "26       15.61400  32.4907  2112280  2124907      456464175       26   \n",
      "27       15.62060  32.4876  2112412  2124876      456489226       27   \n",
      "28       15.60670  32.5135  2112134  2125135      456618179       28   \n",
      "29       15.60270  32.5362  2112054  2125362      456657432       29   \n",
      "...           ...      ...      ...      ...            ...      ...   \n",
      "1157540  12.02210  75.2573  2040442  2552573  4309245258397  1157540   \n",
      "1157541  12.03700  75.3591  2040740  2553591  4309246491525  1157541   \n",
      "1157542  12.02700  75.5177  2040540  2555177  4309260918161  1157542   \n",
      "1157543  11.98760  75.3108  2039752  2553108  4309266658160  1157543   \n",
      "1157544  11.87170  75.3680  2037434  2553680  4309270099140  1157544   \n",
      "1157545  11.87480  75.3738  2037495  2553738  4309270107603  1157545   \n",
      "1157546  11.71070  75.5313  2034214  2555313  4309281917207  1157546   \n",
      "1157547  12.76880  44.9902  2055376  2249902  4335908560382  1157547   \n",
      "1157548  11.61930  43.1457  2032386  2231457  4336490234887  1157548   \n",
      "1157549  11.61760  43.1454  2032352  2231454  4336490265942  1157549   \n",
      "1157550   8.17263  40.1197  1963452  2201197  4364702306571  1157550   \n",
      "1157551   7.66769  36.8176  1953353  2168176  4365220283843  1157551   \n",
      "1157552   4.83896  31.5934  1896779  2115934  4376290795931  1157552   \n",
      "1157553   9.10091  32.2720  1982018  2122720  4381335602860  1157553   \n",
      "1157554   9.09994  32.2711  1981998  2122711  4381335615299  1157554   \n",
      "1157555   8.99212  38.7718  1979842  2187718  4386018467890  1157555   \n",
      "1157556   8.99516  38.7775  1979903  2187775  4386018470570  1157556   \n",
      "1157557   8.99493  38.7754  1979898  2187754  4386018470690  1157557   \n",
      "1157558   9.00013  38.7414  1980002  2187414  4386018590002  1157558   \n",
      "1157559   9.00523  38.7814  1980104  2187814  4386018882684  1157559   \n",
      "1157560   9.00626  38.7811  1980125  2187811  4386018883572  1157560   \n",
      "1157561   8.98351  38.7961  1979670  2187961  4386019198589  1157561   \n",
      "1157562   8.98305  38.7969  1979661  2187969  4386019211864  1157562   \n",
      "1157563   9.03033  38.7540  1980606  2187540  4386032567908  1157563   \n",
      "1157564   9.03042  38.7524  1980608  2187524  4386032581274  1157564   \n",
      "1157565   9.03116  38.7499  1980623  2187499  4386032584208  1157565   \n",
      "1157566   9.00391  38.7210  1980078  2187210  4386038297778  1157566   \n",
      "1157567  12.61250  37.4686  2052250  2174686  4389959668402  1157567   \n",
      "1157568  11.97790  38.9841  2039558  2189841  4390490013975  1157568   \n",
      "1157569  13.69440  30.3701  2073888  2103701  4397103990705  1157569   \n",
      "\n",
      "         prediction  \n",
      "0               NaN  \n",
      "1               NaN  \n",
      "2               NaN  \n",
      "3               NaN  \n",
      "4               NaN  \n",
      "5               NaN  \n",
      "6               NaN  \n",
      "7               NaN  \n",
      "8               NaN  \n",
      "9               NaN  \n",
      "10              NaN  \n",
      "11              NaN  \n",
      "12              NaN  \n",
      "13              NaN  \n",
      "14              NaN  \n",
      "15              NaN  \n",
      "16              NaN  \n",
      "17              NaN  \n",
      "18              NaN  \n",
      "19              NaN  \n",
      "20              NaN  \n",
      "21              NaN  \n",
      "22              NaN  \n",
      "23              NaN  \n",
      "24              NaN  \n",
      "25              NaN  \n",
      "26              NaN  \n",
      "27              NaN  \n",
      "28              NaN  \n",
      "29              NaN  \n",
      "...             ...  \n",
      "1157540         NaN  \n",
      "1157541         NaN  \n",
      "1157542         NaN  \n",
      "1157543         NaN  \n",
      "1157544         NaN  \n",
      "1157545         NaN  \n",
      "1157546         NaN  \n",
      "1157547         NaN  \n",
      "1157548         NaN  \n",
      "1157549         NaN  \n",
      "1157550         NaN  \n",
      "1157551         NaN  \n",
      "1157552         NaN  \n",
      "1157553         NaN  \n",
      "1157554         NaN  \n",
      "1157555         NaN  \n",
      "1157556         NaN  \n",
      "1157557         NaN  \n",
      "1157558         NaN  \n",
      "1157559         NaN  \n",
      "1157560         NaN  \n",
      "1157561         NaN  \n",
      "1157562         NaN  \n",
      "1157563         NaN  \n",
      "1157564         NaN  \n",
      "1157565         NaN  \n",
      "1157566         NaN  \n",
      "1157567         NaN  \n",
      "1157568         NaN  \n",
      "1157569         NaN  \n",
      "\n",
      "[1157570 rows x 7 columns]\n",
      "                x        y       x_       y_       hilbert    order  \\\n",
      "0        15.24060  32.4770  2104812  2124770  3.248475e-16        0   \n",
      "1        15.55460  32.5908  2111092  2125908  3.664225e-16        1   \n",
      "2        15.60510  32.6348  2112102  2126348  3.777531e-16        2   \n",
      "3        15.59940  32.5877  2111988  2125877  3.782683e-16        3   \n",
      "4        15.60170  32.5865  2112034  2125865  3.782713e-16        4   \n",
      "5        15.60190  32.5874  2112038  2125874  3.782720e-16        5   \n",
      "6        15.61570  32.6319  2112314  2126319  3.783989e-16        6   \n",
      "7        15.65180  32.5395  2113036  2125395  3.795152e-16        7   \n",
      "8        15.63280  32.5325  2112656  2125325  3.797385e-16        8   \n",
      "9        15.64080  32.5314  2112816  2125314  3.797547e-16        9   \n",
      "10       15.64590  32.5313  2112918  2125313  3.798019e-16       10   \n",
      "11       15.64210  32.4882  2112842  2124882  3.799783e-16       11   \n",
      "12       15.64080  32.4881  2112816  2124881  3.799823e-16       12   \n",
      "13       15.63880  32.4877  2112776  2124877  3.799835e-16       13   \n",
      "14       15.64420  32.4944  2112884  2124944  3.799980e-16       14   \n",
      "15       15.66340  32.5070  2113268  2125070  3.801355e-16       15   \n",
      "16       15.64980  32.3915  2112996  2123915  3.807528e-16       16   \n",
      "17       15.64490  32.4704  2112898  2124704  3.810421e-16       17   \n",
      "18       15.61290  32.4547  2112258  2124547  3.811602e-16       18   \n",
      "19       15.61830  32.4712  2112366  2124712  3.811701e-16       19   \n",
      "20       15.58600  32.5286  2111720  2125286  3.820928e-16       20   \n",
      "21       15.58610  32.5310  2111722  2125310  3.820930e-16       21   \n",
      "22       15.58690  32.5263  2111738  2125263  3.820943e-16       22   \n",
      "23       15.59900  32.5239  2111980  2125239  3.821500e-16       23   \n",
      "24       15.60300  32.4970  2112060  2124970  3.822371e-16       24   \n",
      "25       15.60730  32.4999  2112146  2124999  3.822524e-16       25   \n",
      "26       15.61400  32.4907  2112280  2124907  3.822812e-16       26   \n",
      "27       15.62060  32.4876  2112412  2124876  3.823021e-16       27   \n",
      "28       15.60670  32.5135  2112134  2125135  3.824101e-16       28   \n",
      "29       15.60270  32.5362  2112054  2125362  3.824430e-16       29   \n",
      "...           ...      ...      ...      ...           ...      ...   \n",
      "1157540  12.02210  75.2573  2040442  2552573  3.608921e-12  1157540   \n",
      "1157541  12.03700  75.3591  2040740  2553591  3.608922e-12  1157541   \n",
      "1157542  12.02700  75.5177  2040540  2555177  3.608934e-12  1157542   \n",
      "1157543  11.98760  75.3108  2039752  2553108  3.608939e-12  1157543   \n",
      "1157544  11.87170  75.3680  2037434  2553680  3.608942e-12  1157544   \n",
      "1157545  11.87480  75.3738  2037495  2553738  3.608942e-12  1157545   \n",
      "1157546  11.71070  75.5313  2034214  2555313  3.608952e-12  1157546   \n",
      "1157547  12.76880  44.9902  2055376  2249902  3.631251e-12  1157547   \n",
      "1157548  11.61930  43.1457  2032386  2231457  3.631739e-12  1157548   \n",
      "1157549  11.61760  43.1454  2032352  2231454  3.631739e-12  1157549   \n",
      "1157550   8.17263  40.1197  1963452  2201197  3.655366e-12  1157550   \n",
      "1157551   7.66769  36.8176  1953353  2168176  3.655799e-12  1157551   \n",
      "1157552   4.83896  31.5934  1896779  2115934  3.665071e-12  1157552   \n",
      "1157553   9.10091  32.2720  1982018  2122720  3.669296e-12  1157553   \n",
      "1157554   9.09994  32.2711  1981998  2122711  3.669296e-12  1157554   \n",
      "1157555   8.99212  38.7718  1979842  2187718  3.673218e-12  1157555   \n",
      "1157556   8.99516  38.7775  1979903  2187775  3.673218e-12  1157556   \n",
      "1157557   8.99493  38.7754  1979898  2187754  3.673218e-12  1157557   \n",
      "1157558   9.00013  38.7414  1980002  2187414  3.673218e-12  1157558   \n",
      "1157559   9.00523  38.7814  1980104  2187814  3.673218e-12  1157559   \n",
      "1157560   9.00626  38.7811  1980125  2187811  3.673218e-12  1157560   \n",
      "1157561   8.98351  38.7961  1979670  2187961  3.673218e-12  1157561   \n",
      "1157562   8.98305  38.7969  1979661  2187969  3.673218e-12  1157562   \n",
      "1157563   9.03033  38.7540  1980606  2187540  3.673229e-12  1157563   \n",
      "1157564   9.03042  38.7524  1980608  2187524  3.673229e-12  1157564   \n",
      "1157565   9.03116  38.7499  1980623  2187499  3.673229e-12  1157565   \n",
      "1157566   9.00391  38.7210  1980078  2187210  3.673234e-12  1157566   \n",
      "1157567  12.61250  37.4686  2052250  2174686  3.676518e-12  1157567   \n",
      "1157568  11.97790  38.9841  2039558  2189841  3.676962e-12  1157568   \n",
      "1157569  13.69440  30.3701  2073888  2103701  3.682502e-12  1157569   \n",
      "\n",
      "         prediction  \n",
      "0               NaN  \n",
      "1               NaN  \n",
      "2               NaN  \n",
      "3               NaN  \n",
      "4               NaN  \n",
      "5               NaN  \n",
      "6               NaN  \n",
      "7               NaN  \n",
      "8               NaN  \n",
      "9               NaN  \n",
      "10              NaN  \n",
      "11              NaN  \n",
      "12              NaN  \n",
      "13              NaN  \n",
      "14              NaN  \n",
      "15              NaN  \n",
      "16              NaN  \n",
      "17              NaN  \n",
      "18              NaN  \n",
      "19              NaN  \n",
      "20              NaN  \n",
      "21              NaN  \n",
      "22              NaN  \n",
      "23              NaN  \n",
      "24              NaN  \n",
      "25              NaN  \n",
      "26              NaN  \n",
      "27              NaN  \n",
      "28              NaN  \n",
      "29              NaN  \n",
      "...             ...  \n",
      "1157540         NaN  \n",
      "1157541         NaN  \n",
      "1157542         NaN  \n",
      "1157543         NaN  \n",
      "1157544         NaN  \n",
      "1157545         NaN  \n",
      "1157546         NaN  \n",
      "1157547         NaN  \n",
      "1157548         NaN  \n",
      "1157549         NaN  \n",
      "1157550         NaN  \n",
      "1157551         NaN  \n",
      "1157552         NaN  \n",
      "1157553         NaN  \n",
      "1157554         NaN  \n",
      "1157555         NaN  \n",
      "1157556         NaN  \n",
      "1157557         NaN  \n",
      "1157558         NaN  \n",
      "1157559         NaN  \n",
      "1157560         NaN  \n",
      "1157561         NaN  \n",
      "1157562         NaN  \n",
      "1157563         NaN  \n",
      "1157564         NaN  \n",
      "1157565         NaN  \n",
      "1157566         NaN  \n",
      "1157567         NaN  \n",
      "1157568         NaN  \n",
      "1157569         NaN  \n",
      "\n",
      "[1157570 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./Data/HilbertSortedPOIs.csv\")\n",
    "print(df)\n",
    "TOTAL = len(df)+1\n",
    "COL = df.columns\n",
    "\n",
    "# maybe I should substract the mean of hilbert: not performance gain\n",
    "# print(df.iloc[:,4].mean())\n",
    "# df.iloc[:,4] = df.iloc[:,4] - df.iloc[:,4].mean()\n",
    "# print(df)\n",
    "\n",
    "# maybe I should divide the variance of hilbert? \n",
    "df.iloc[:,4] = df.iloc[:,4] / df.iloc[:,4].var()\n",
    "print(df)\n",
    "\n",
    "# 用GAN 来做类似Rtree？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "[  2.04740792e+01   8.85068347e+00   2.20948145e+06   1.88850673e+06\n",
      "   1.52127265e-12]\n",
      "                x         y        x_        y_   hilbert    order  prediction\n",
      "0       -0.206655  0.278680 -0.206655  0.278680 -1.661982        0         NaN\n",
      "1       -0.194256  0.280023 -0.194256  0.280023 -1.661937        1         NaN\n",
      "2       -0.192262  0.280542 -0.192262  0.280542 -1.661924        2         NaN\n",
      "3       -0.192487  0.279986 -0.192487  0.279986 -1.661924        3         NaN\n",
      "4       -0.192396  0.279972 -0.192396  0.279972 -1.661924        4         NaN\n",
      "5       -0.192388  0.279982 -0.192388  0.279983 -1.661924        5         NaN\n",
      "6       -0.191843  0.280507 -0.191843  0.280507 -1.661924        6         NaN\n",
      "7       -0.190418  0.279417 -0.190417  0.279418 -1.661923        7         NaN\n",
      "8       -0.191168  0.279335 -0.191168  0.279335 -1.661922        8         NaN\n",
      "9       -0.190852  0.279322 -0.190852  0.279322 -1.661922        9         NaN\n",
      "10      -0.190651  0.279321 -0.190650  0.279321 -1.661922       10         NaN\n",
      "11      -0.190801  0.278812 -0.190800  0.278812 -1.661922       11         NaN\n",
      "12      -0.190852  0.278811 -0.190852  0.278811 -1.661922       12         NaN\n",
      "13      -0.190931  0.278806 -0.190931  0.278807 -1.661922       13         NaN\n",
      "14      -0.190718  0.278885 -0.190718  0.278886 -1.661922       14         NaN\n",
      "15      -0.189960  0.279034 -0.189959  0.279034 -1.661922       15         NaN\n",
      "16      -0.190497  0.277672 -0.190496  0.277672 -1.661921       16         NaN\n",
      "17      -0.190690  0.278602 -0.190690  0.278603 -1.661921       17         NaN\n",
      "18      -0.191954  0.278417 -0.191954  0.278417 -1.661921       18         NaN\n",
      "19      -0.191741  0.278612 -0.191740  0.278612 -1.661921       19         NaN\n",
      "20      -0.193016  0.279289 -0.193016  0.279289 -1.661920       20         NaN\n",
      "21      -0.193012  0.279317 -0.193012  0.279317 -1.661920       21         NaN\n",
      "22      -0.192980  0.279262 -0.192980  0.279262 -1.661920       22         NaN\n",
      "23      -0.192503  0.279233 -0.192502  0.279234 -1.661920       23         NaN\n",
      "24      -0.192345  0.278916 -0.192344  0.278916 -1.661920       24         NaN\n",
      "25      -0.192175  0.278950 -0.192175  0.278950 -1.661920       25         NaN\n",
      "26      -0.191910  0.278842 -0.191910  0.278842 -1.661920       26         NaN\n",
      "27      -0.191650  0.278805 -0.191649  0.278805 -1.661920       27         NaN\n",
      "28      -0.192199  0.279111 -0.192198  0.279111 -1.661919       28         NaN\n",
      "29      -0.192357  0.279379 -0.192356  0.279379 -1.661919       29         NaN\n",
      "...           ...       ...       ...       ...       ...      ...         ...\n",
      "1157540 -0.333744  0.783288 -0.333744  0.783288  2.281232  1157540         NaN\n",
      "1157541 -0.333156  0.784489 -0.333155  0.784489  2.281233  1157541         NaN\n",
      "1157542 -0.333551  0.786360 -0.333550  0.786360  2.281247  1157542         NaN\n",
      "1157543 -0.335106  0.783919 -0.335106  0.783919  2.281252  1157543         NaN\n",
      "1157544 -0.339683  0.784594 -0.339683  0.784594  2.281255  1157544         NaN\n",
      "1157545 -0.339560  0.784662 -0.339562  0.784662  2.281255  1157545         NaN\n",
      "1157546 -0.346040  0.786520 -0.346040  0.786520  2.281266  1157546         NaN\n",
      "1157547 -0.304259  0.426278 -0.304259  0.426278  2.305633  1157547         NaN\n",
      "1157548 -0.349649  0.404521 -0.349649  0.404521  2.306165  1157548         NaN\n",
      "1157549 -0.349717  0.404518 -0.349716  0.404518  2.306165  1157549         NaN\n",
      "1157550 -0.485748  0.368828 -0.485749  0.368829  2.331983  1157550         NaN\n",
      "1157551 -0.505687  0.329879 -0.505688  0.329879  2.332457  1157551         NaN\n",
      "1157552 -0.617385  0.268258 -0.617385  0.268258  2.342588  1157552         NaN\n",
      "1157553 -0.449093  0.276262 -0.449093  0.276262  2.347205  1157553         NaN\n",
      "1157554 -0.449132  0.276252 -0.449133  0.276252  2.347205  1157554         NaN\n",
      "1157555 -0.453389  0.352930 -0.453390  0.352930  2.351491  1157555         NaN\n",
      "1157556 -0.453269  0.352997 -0.453269  0.352997  2.351491  1157556         NaN\n",
      "1157557 -0.453278  0.352972 -0.453279  0.352972  2.351491  1157557         NaN\n",
      "1157558 -0.453073  0.352571 -0.453074  0.352571  2.351491  1157558         NaN\n",
      "1157559 -0.452871  0.353043 -0.452872  0.353043  2.351491  1157559         NaN\n",
      "1157560 -0.452831  0.353039 -0.452831  0.353039  2.351491  1157560         NaN\n",
      "1157561 -0.453729  0.353216 -0.453729  0.353216  2.351491  1157561         NaN\n",
      "1157562 -0.453747  0.353226 -0.453747  0.353226  2.351491  1157562         NaN\n",
      "1157563 -0.451880  0.352720 -0.451881  0.352720  2.351504  1157563         NaN\n",
      "1157564 -0.451877  0.352701 -0.451877  0.352701  2.351504  1157564         NaN\n",
      "1157565 -0.451848  0.352671 -0.451848  0.352671  2.351504  1157565         NaN\n",
      "1157566 -0.452924  0.352330 -0.452924  0.352330  2.351509  1157566         NaN\n",
      "1157567 -0.310431  0.337558 -0.310431  0.337558  2.355097  1157567         NaN\n",
      "1157568 -0.335489  0.355434 -0.335489  0.355434  2.355583  1157568         NaN\n",
      "1157569 -0.267710  0.253829 -0.267709  0.253829  2.361635  1157569         NaN\n",
      "\n",
      "[1157570 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.any(np.isnan(df))) # should be false\n",
    "print(np.all(np.isfinite(df))) # should be true\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "print(scaler.fit(df.iloc[:,0:5]))\n",
    "print(scaler.mean_)\n",
    "df.iloc[:,0:5]=scaler.transform(df.iloc[:,0:5])\n",
    "print(df)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======stage:0======\n",
      "model:0\n",
      "======stage:1======\n",
      "model:0\n",
      "model:1\n",
      "model:2\n",
      "2\n",
      "model:3\n",
      "model:4\n",
      "model:5\n",
      "model:6\n",
      "model:7\n",
      "7\n",
      "model:8\n",
      "model:9\n",
      "======stage:2======\n",
      "model:0\n",
      "model:1\n",
      "model:2\n",
      "model:3\n",
      "model:4\n",
      "model:5\n",
      "model:6\n",
      "model:7\n",
      "model:8\n",
      "model:9\n",
      "model:10\n",
      "model:11\n",
      "model:12\n",
      "12\n",
      "model:13\n",
      "model:14\n",
      "model:15\n",
      "15\n",
      "model:16\n",
      "model:17\n",
      "17\n",
      "model:18\n",
      "model:19\n",
      "model:20\n",
      "20\n",
      "model:21\n",
      "model:22\n",
      "model:23\n",
      "23\n",
      "model:24\n",
      "model:25\n",
      "model:26\n",
      "model:27\n",
      "model:28\n",
      "model:29\n",
      "model:30\n",
      "model:31\n",
      "model:32\n",
      "model:33\n",
      "model:34\n",
      "model:35\n",
      "model:36\n",
      "model:37\n",
      "37\n",
      "model:38\n",
      "model:39\n",
      "model:40\n",
      "model:41\n",
      "model:42\n",
      "model:43\n",
      "model:44\n",
      "model:45\n",
      "model:46\n",
      "model:47\n",
      "model:48\n",
      "model:49\n",
      "model:50\n",
      "50\n",
      "model:51\n",
      "model:52\n",
      "model:53\n",
      "53\n",
      "model:54\n",
      "model:55\n",
      "model:56\n",
      "56\n",
      "model:57\n",
      "model:58\n",
      "model:59\n",
      "59\n",
      "model:60\n",
      "model:61\n",
      "model:62\n",
      "62\n",
      "model:63\n",
      "model:64\n",
      "64\n",
      "model:65\n",
      "model:66\n",
      "model:67\n",
      "67\n",
      "model:68\n",
      "model:69\n",
      "model:70\n",
      "70\n",
      "model:71\n",
      "model:72\n",
      "model:73\n",
      "73\n",
      "model:74\n",
      "model:75\n",
      "model:76\n",
      "76\n",
      "model:77\n",
      "model:78\n",
      "78\n",
      "model:79\n",
      "model:80\n",
      "model:81\n",
      "81\n",
      "model:82\n",
      "model:83\n",
      "model:84\n",
      "84\n",
      "model:85\n",
      "model:86\n",
      "model:87\n",
      "87\n",
      "model:88\n",
      "model:89\n",
      "model:90\n",
      "90\n",
      "model:91\n",
      "model:92\n",
      "model:93\n",
      "93\n",
      "model:94\n",
      "model:95\n",
      "95\n",
      "model:96\n",
      "model:97\n",
      "model:98\n",
      "98\n",
      "model:99\n",
      "95.75661325454712\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "import ipdb\n",
    "import time\n",
    "stages = [1,10,100]\n",
    "index = []\n",
    "data = []\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(len(stages)):\n",
    "    temp = []\n",
    "    for j in range(stages[i]):\n",
    "        df_ = pd.DataFrame(columns=COL)\n",
    "        temp.append(df_)\n",
    "    data.append(temp)\n",
    "    \n",
    "data[0][0] = df\n",
    "nodatacount = 0\n",
    "\n",
    "for i in range(len(stages)):\n",
    "    temp = []\n",
    "    print(\"======stage:\"+str(i)+\"======\")\n",
    "    \n",
    "    for j in range(stages[i]):\n",
    "        print(\"model:\"+str(j))\n",
    "        if len(data[i][j]) == 0:\n",
    "            nodatacount += 1\n",
    "            data[i][j] = data[i][j-1]\n",
    "            print(j)\n",
    "#         lr = linear_model.LinearRegression()\n",
    "#         lr = linear_model.ElasticNet(max_iter=10000)\n",
    "#         lr = linear_model.Lasso(max_iter=10000)\n",
    "#         lr = ensemble.GradientBoostingRegressor()\n",
    "        lr = ensemble.AdaBoostRegressor()\n",
    "#         train_x = data[i][j].iloc[:,4].reshape(-1,1)\n",
    "        train_x = data[i][j].iloc[:,0:5]\n",
    "        train_y = data[i][j].iloc[:,5]\n",
    "        lr.fit(train_x,train_y)\n",
    "        temp.append(lr)\n",
    "        # allocate data\n",
    "        if i < len(stages)-1:\n",
    "            predicted = lr.predict(train_x)\n",
    "            for pi in range(len(predicted)): # do not change the prediction\n",
    "                #print(item)\n",
    "                if predicted[pi] < 0:\n",
    "                    predicted[pi] = 0\n",
    "                if predicted[pi] >= TOTAL:\n",
    "                    predicted[pi] = TOTAL-1   \n",
    "            ModelID = ((predicted/TOTAL)*stages[i+1]).astype(int)\n",
    "            data[i][j].iloc[:,-1]=ModelID\n",
    "#             if i == 1 and np.any(data[i][j]['prediction'] == 4):\n",
    "#                 print(j)\n",
    "\n",
    "            for k in range(stages[i+1]):\n",
    "                data[i+1][k] = data[i+1][k].append(data[i][j].loc[data[i][j]['prediction'] == k])\n",
    "    index.append(temp)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "print(nodatacount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# use the generated stage index for prediction\n",
    "def predict_with_stage_model(model, data):\n",
    "    totalSize = len(data)+1\n",
    "    predicted_y = []\n",
    "    tempy = 0\n",
    "    for i in range(len(data)):\n",
    "        modelIndex = 0\n",
    "        for m in range(len(model)):\n",
    "            tempy = model[m][modelIndex].predict(data.iloc[i,:].reshape(1,-1))\n",
    "#             tempy = model[m][modelIndex].predict(data.iloc[i,])\n",
    "            if tempy < 0:\n",
    "                tempy = 0\n",
    "            if tempy >= totalSize:\n",
    "                tempy = totalSize-1\n",
    "            if m < len(model)-1:\n",
    "                modelIndex = int((tempy/totalSize)*len(model[m+1]))\n",
    "                \n",
    "        predicted_y.append(tempy)\n",
    "        \n",
    "    return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1760.7233381271362\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAEKCAYAAABzHwA5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzgbmG0QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef46486b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "train_x=df.iloc[:,0:5]\n",
    "# train_x=df.iloc[:,4]\n",
    "train_y=df.iloc[:,5]\n",
    "\n",
    "start_time=time.time()\n",
    "predicted_y = predict_with_stage_model(index, train_x)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(train_y, predicted_y, edgecolors=(0, 0, 0))\n",
    "ax.plot([train_y.min(), train_y.max()], [train_y.min(), train_y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1157570\n",
      "1157570\n",
      "0.999996864371\n",
      "[ 461.06116996]\n"
     ]
    }
   ],
   "source": [
    "# estimate the result\n",
    "from sklearn.metrics import explained_variance_score\n",
    "train_y=df.iloc[:,5]\n",
    "print(len(train_y))\n",
    "print(len(predicted_y))\n",
    "\n",
    "# 1- Var(Ytrue - Ypred)/Var(Ytrue)  best: 1  the lower the worse\n",
    "result = explained_variance_score(train_y, predicted_y)\n",
    "print(result)# calculate scan item amount\n",
    "\n",
    "ind = 0\n",
    "scan = 0\n",
    "totalscan = 0\n",
    "for i in range(len(df)):\n",
    "    ind = predicted_y[i]\n",
    "    scan = abs(ind-i) # distance to exact\n",
    "    totalscan += scan\n",
    "\n",
    "print(totalscan/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
