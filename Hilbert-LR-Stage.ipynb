{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               x         y       x_       y_         hilbert    order  \\\n",
      "0       -54.9356  -67.6150   701288  1123850   2121205890180        0   \n",
      "1       -54.7979  -68.3110   704042  1116890   2121429835144        1   \n",
      "2       -54.8064  -68.3336   703872  1116664   2121430220778        2   \n",
      "3       -54.8080  -68.3086   703840  1116914   2121430582030        3   \n",
      "4       -54.8080  -68.3074   703840  1116926   2121430582270        4   \n",
      "5       -54.8075  -68.3126   703850  1116874   2121430584456        5   \n",
      "6       -54.8075  -68.3034   703850  1116966   2121430610552        6   \n",
      "7       -54.8061  -68.3032   703878  1116968   2121430615254        7   \n",
      "8       -54.8012  -68.3056   703976  1116944   2121430620010        8   \n",
      "9       -54.8045  -68.3008   703910  1116992   2121430627990        9   \n",
      "10      -54.8062  -68.2972   703876  1117028   2121430629408       10   \n",
      "11      -54.8396  -68.3124   703208  1116876   2121430863034       11   \n",
      "12      -53.7941  -67.7364   724118  1122636   2121786317678       12   \n",
      "13      -53.7818  -67.7242   724364  1122758   2121786364108       13   \n",
      "14      -53.7624  -67.7370   724752  1122630   2121786593556       14   \n",
      "15      -53.7703  -67.7266   724594  1122734   2121786710520       15   \n",
      "16      -53.7811  -67.7120   724378  1122880   2121787077572       16   \n",
      "17      -53.7863  -67.7074   724274  1122926   2121787095544       17   \n",
      "18      -53.7948  -67.7086   724104  1122914   2121787145836       18   \n",
      "19      -53.7948  -67.7086   724104  1122914   2121787145836       19   \n",
      "20      -53.3255  -68.2656   733490  1117344   2121970869006       20   \n",
      "21      -52.3160  -69.6864   753680  1103136   2126834765226       21   \n",
      "22      -51.6205  -69.2198   767590  1107802   2126967810008       22   \n",
      "23      -51.6195  -69.4048   767610  1105952   2126969057638       23   \n",
      "24      -52.7767  -69.2908   744466  1107092   2127307008156       24   \n",
      "25      -53.1417  -70.8754   737166  1091246   2127640471042       25   \n",
      "26      -53.1412  -70.8790   737176  1091210   2127640472462       26   \n",
      "27      -53.1366  -70.8860   737268  1091140   2127640481674       27   \n",
      "28      -53.1376  -70.8874   737248  1091126   2127640482198       28   \n",
      "29      -53.1384  -70.8924   737232  1091076   2127640484272       29   \n",
      "...          ...       ...      ...      ...             ...      ...   \n",
      "1157540 -37.6819  176.1670  1046362  3561670  16945383859157  1157540   \n",
      "1157541 -37.6823  176.1700  1046354  3561699  16945383861680  1157541   \n",
      "1157542 -37.6829  176.1700  1046342  3561699  16945383861920  1157542   \n",
      "1157543 -37.7016  176.1590  1045968  3561590  16945384080879  1157543   \n",
      "1157544 -37.7119  176.1510  1045762  3561510  16945384107067  1157544   \n",
      "1157545 -37.7134  176.1510  1045732  3561510  16945384149665  1157545   \n",
      "1157546 -37.7127  176.1520  1045746  3561520  16945384149849  1157546   \n",
      "1157547 -37.7362  176.1900  1045276  3561900  16945384810381  1157547   \n",
      "1157548 -37.8881  176.7570  1042238  3567570  16945410366555  1157548   \n",
      "1157549 -37.9499  177.0020  1041002  3570020  16945467196111  1157549   \n",
      "1157550 -37.9508  176.9940  1040984  3569940  16945467208243  1157550   \n",
      "1157551 -37.9780  177.0900  1040440  3570900  16945470174397  1157551   \n",
      "1157552 -38.2854  176.3870  1034292  3563870  16945511079159  1157552   \n",
      "1157553 -38.6979  176.0830  1026042  3560829  16945657954358  1157553   \n",
      "1157554 -38.6248  176.0900  1027504  3560900  16945661517555  1157554   \n",
      "1157555 -38.6835  176.0770  1026330  3560770  16945662958571  1157555   \n",
      "1157556 -38.6868  176.0660  1026264  3560660  16945663051965  1157556   \n",
      "1157557 -38.6886  176.0700  1026228  3560700  16945663053923  1157557   \n",
      "1157558 -38.6899  176.0720  1026202  3560720  16945663132097  1157558   \n",
      "1157559 -38.6892  176.0750  1026216  3560750  16945663133905  1157559   \n",
      "1157560 -38.6886  176.0740  1026228  3560740  16945663134627  1157560   \n",
      "1157561 -38.6860  176.0710  1026280  3560710  16945663136809  1157561   \n",
      "1157562 -38.8562  176.0140  1022876  3560140  16945685051683  1157562   \n",
      "1157563 -39.2010  175.5400  1015980  3555399  16945716557012  1157563   \n",
      "1157564 -37.8916  178.3180  1042168  3583180  16946215915389  1157564   \n",
      "1157565 -38.1065  178.3520  1037870  3583519  16946307796234  1157565   \n",
      "1157566 -38.3719  178.2970  1032562  3582970  16946322369093  1157566   \n",
      "1157567 -38.3719  178.2970  1032562  3582970  16946322369093  1157567   \n",
      "1157568 -38.3821  178.3170  1032358  3583170  16946325756987  1157568   \n",
      "1157569 -76.3099   22.4995   273802  2024995  17344470968016  1157569   \n",
      "\n",
      "         prediction  \n",
      "0               NaN  \n",
      "1               NaN  \n",
      "2               NaN  \n",
      "3               NaN  \n",
      "4               NaN  \n",
      "5               NaN  \n",
      "6               NaN  \n",
      "7               NaN  \n",
      "8               NaN  \n",
      "9               NaN  \n",
      "10              NaN  \n",
      "11              NaN  \n",
      "12              NaN  \n",
      "13              NaN  \n",
      "14              NaN  \n",
      "15              NaN  \n",
      "16              NaN  \n",
      "17              NaN  \n",
      "18              NaN  \n",
      "19              NaN  \n",
      "20              NaN  \n",
      "21              NaN  \n",
      "22              NaN  \n",
      "23              NaN  \n",
      "24              NaN  \n",
      "25              NaN  \n",
      "26              NaN  \n",
      "27              NaN  \n",
      "28              NaN  \n",
      "29              NaN  \n",
      "...             ...  \n",
      "1157540         NaN  \n",
      "1157541         NaN  \n",
      "1157542         NaN  \n",
      "1157543         NaN  \n",
      "1157544         NaN  \n",
      "1157545         NaN  \n",
      "1157546         NaN  \n",
      "1157547         NaN  \n",
      "1157548         NaN  \n",
      "1157549         NaN  \n",
      "1157550         NaN  \n",
      "1157551         NaN  \n",
      "1157552         NaN  \n",
      "1157553         NaN  \n",
      "1157554         NaN  \n",
      "1157555         NaN  \n",
      "1157556         NaN  \n",
      "1157557         NaN  \n",
      "1157558         NaN  \n",
      "1157559         NaN  \n",
      "1157560         NaN  \n",
      "1157561         NaN  \n",
      "1157562         NaN  \n",
      "1157563         NaN  \n",
      "1157564         NaN  \n",
      "1157565         NaN  \n",
      "1157566         NaN  \n",
      "1157567         NaN  \n",
      "1157568         NaN  \n",
      "1157569         NaN  \n",
      "\n",
      "[1157570 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./Data/HilbertSortedPOIs.csv\")\n",
    "print(df)\n",
    "TOTAL = len(df)+1\n",
    "COL = df.columns\n",
    "\n",
    "# maybe I should substract the mean of hilbert: not performance gain\n",
    "# print(df.iloc[:,4].mean())\n",
    "# df.iloc[:,4] = df.iloc[:,4] - df.iloc[:,4].mean()\n",
    "# print(df)|\n",
    "\n",
    "# maybe I should divide the variance of hilbert? \n",
    "# df.iloc[:,4] = df.iloc[:,4] / df.iloc[:,4].var()\n",
    "# print(df.loc[\"hilbert\"]==2121205890180)\n",
    "# print(df.loc[df[\"order\"]==1].x == -54.7979)\n",
    "# 用GAN 来做类似Rtree？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "[  2.04740792e+01   8.85068347e+00   2.20948145e+06   1.88850673e+06\n",
      "   7.45205203e-13]\n",
      "          x         y        x_        y_   hilbert  order  prediction\n",
      "0 -2.977709 -0.901938 -2.977708 -0.901938 -2.103736      0         NaN\n",
      "2 -2.972607 -0.910414 -2.972606 -0.910414 -2.103674      2         NaN\n",
      "5 -2.972650 -0.910167 -2.972650 -0.910166 -2.103674      5         NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.any(np.isnan(df))) # should be false\n",
    "print(np.all(np.isfinite(df))) # should be true\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "print(scaler.fit(df.iloc[:,0:5]))\n",
    "print(scaler.mean_)\n",
    "df.iloc[:,0:5]=scaler.transform(df.iloc[:,0:5])\n",
    "# print(df)\n",
    "# print(df)\n",
    "selected = [0,2,5]\n",
    "print(df.iloc[selected,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn import ensemble\n",
    "# lr = ensemble.GradientBoostingRegressor()\n",
    "# train_x = df.iloc[:,0:5]\n",
    "# train_y = df.iloc[:,5]\n",
    "# start_time=time.time()\n",
    "# lr.fit(train_x,train_y)\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(elapsed_time)\n",
    "\n",
    "# start_time=time.time()\n",
    "# predicted = lr.predict(train_x)\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(elapsed_time)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.scatter(train_y, predicted, edgecolors=(0, 0, 0))\n",
    "# ax.plot([train_y.min(), train_y.max()], [train_y.min(), train_y.max()], 'k--', lw=4)\n",
    "# ax.set_xlabel('Measured')\n",
    "# ax.set_ylabel('Predicted')\n",
    "# plt.show()\n",
    "\n",
    "# from sklearn.metrics import explained_variance_score\n",
    "# train_y=df.iloc[:,5]\n",
    "# print(len(train_y))\n",
    "# print(len(predicted))\n",
    "\n",
    "# # 1- Var(Ytrue - Ypred)/Var(Ytrue)  best: 1  the lower the worse\n",
    "# result = explained_variance_score(train_y, predicted)\n",
    "# print(result)# calculate scan item amount\n",
    "\n",
    "# ind = 0\n",
    "# scan = 0\n",
    "# totalscan = 0\n",
    "# for i in range(len(df)):\n",
    "#     ind = predicted[i]\n",
    "#     scan = abs(ind-i) # distance to exact\n",
    "#     totalscan += scan\n",
    "\n",
    "# print(totalscan/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======stage:0======\n",
      "model:0\n",
      "======stage:1======\n",
      "model:0\n",
      "model:1\n",
      "model:2\n",
      "model:3\n",
      "model:4\n",
      "model:5\n",
      "model:6\n",
      "model:7\n",
      "model:8\n",
      "model:9\n",
      "======stage:2======\n",
      "model:0\n",
      "model:1\n",
      "model:2\n",
      "model:3\n",
      "model:4\n",
      "model:5\n",
      "model:6\n",
      "model:7\n",
      "model:8\n",
      "model:9\n",
      "model:10\n",
      "model:11\n",
      "model:12\n",
      "model:13\n",
      "model:14\n",
      "model:15\n",
      "model:16\n",
      "model:17\n",
      "model:18\n",
      "model:19\n",
      "model:20\n",
      "model:21\n",
      "model:22\n",
      "model:23\n",
      "model:24\n",
      "model:25\n",
      "model:26\n",
      "model:27\n",
      "model:28\n",
      "model:29\n",
      "model:30\n",
      "model:31\n",
      "model:32\n",
      "model:33\n",
      "model:34\n",
      "model:35\n",
      "model:36\n",
      "model:37\n",
      "model:38\n",
      "model:39\n",
      "model:40\n",
      "model:41\n",
      "model:42\n",
      "model:43\n",
      "model:44\n",
      "model:45\n",
      "model:46\n",
      "model:47\n",
      "model:48\n",
      "model:49\n",
      "model:50\n",
      "model:51\n",
      "model:52\n",
      "model:53\n",
      "model:54\n",
      "model:55\n",
      "model:56\n",
      "model:57\n",
      "model:58\n",
      "model:59\n",
      "model:60\n",
      "model:61\n",
      "model:62\n",
      "model:63\n",
      "model:64\n",
      "model:65\n",
      "model:66\n",
      "model:67\n",
      "model:68\n",
      "model:69\n",
      "model:70\n",
      "model:71\n",
      "model:72\n",
      "model:73\n",
      "model:74\n",
      "model:75\n",
      "model:76\n",
      "model:77\n",
      "model:78\n",
      "model:79\n",
      "model:80\n",
      "model:81\n",
      "model:82\n",
      "model:83\n",
      "model:84\n",
      "model:85\n",
      "model:86\n",
      "model:87\n",
      "model:88\n",
      "model:89\n",
      "model:90\n",
      "model:91\n",
      "model:92\n",
      "model:93\n",
      "model:94\n",
      "model:95\n",
      "model:96\n",
      "model:97\n",
      "model:98\n",
      "model:99\n",
      "55.890599727630615\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import neural_network\n",
    "from sklearn import ensemble\n",
    "import ipdb\n",
    "import time\n",
    "stages = [1,10,100]\n",
    "index = []\n",
    "data = []\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "for i in range(len(stages)):\n",
    "    temp = []\n",
    "    for j in range(stages[i]):\n",
    "        df_ = pd.DataFrame(columns=COL)\n",
    "        temp.append(df_)\n",
    "    data.append(temp)\n",
    "    \n",
    "data[0][0] = df\n",
    "nodatacount = 0\n",
    "\n",
    "for i in range(len(stages)):\n",
    "    temp = []\n",
    "    print(\"======stage:\"+str(i)+\"======\")\n",
    "    \n",
    "    for j in range(stages[i]):\n",
    "        print(\"model:\"+str(j))\n",
    "        if len(data[i][j]) == 0:\n",
    "            nodatacount += 1\n",
    "            data[i][j] = data[i][j-1]\n",
    "            print(j)\n",
    "#         lr = linear_model.LinearRegression()\n",
    "#         lr = linear_model.ElasticNet(max_iter=10000)\n",
    "#         lr = linear_model.Lasso(max_iter=10000)\n",
    "#         lr = ensemble.GradientBoostingRegressor()\n",
    "#         lr = ensemble.AdaBoostRegressor()\n",
    "        if i == 0:\n",
    "#             lr = neural_network.MLPRegressor(hidden_layer_sizes=(8, 2),max_iter=1000)\n",
    "            lr = ensemble.GradientBoostingRegressor()\n",
    "        else:\n",
    "            lr = ensemble.GradientBoostingRegressor()\n",
    "#         train_x = data[i][j].iloc[:,4].reshape(-1,1)\n",
    "        train_x = data[i][j].iloc[:,2:5]\n",
    "#         train_x = data[i][j].iloc[:,0:5]\n",
    "        train_y = data[i][j].iloc[:,5]\n",
    "        lr.fit(train_x,train_y)\n",
    "        temp.append(lr)\n",
    "        # allocate data\n",
    "        if i < len(stages)-1:\n",
    "            predicted = lr.predict(train_x)\n",
    "            for pi in range(len(predicted)): # do not change the prediction\n",
    "                #print(item)\n",
    "                if predicted[pi] < 0:\n",
    "                    predicted[pi] = 0\n",
    "                if predicted[pi] >= TOTAL:\n",
    "                    predicted[pi] = TOTAL-1   \n",
    "            ModelID = ((predicted/TOTAL)*stages[i+1]).astype(int)\n",
    "            data[i][j].iloc[:,-1]=ModelID\n",
    "#             if i == 1 and np.any(data[i][j]['prediction'] == 4):\n",
    "#                 print(j)\n",
    "\n",
    "            for k in range(stages[i+1]):\n",
    "                data[i+1][k] = data[i+1][k].append(data[i][j].loc[data[i][j]['prediction'] == k])\n",
    "    index.append(temp)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "print(nodatacount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# use the generated stage index for prediction\n",
    "def predict_with_stage_model(model, data):\n",
    "    totalSize = len(data)+1\n",
    "    predicted_y = []\n",
    "    tempy = 0\n",
    "    for i in range(len(data)):\n",
    "        modelIndex = 0\n",
    "        for m in range(len(model)):\n",
    "            tempy = model[m][modelIndex].predict(data.iloc[i,:].reshape(1,-1))\n",
    "#             tempy = model[m][modelIndex].predict(data.iloc[i,])\n",
    "            if tempy < 0:\n",
    "                tempy = 0\n",
    "            if tempy >= totalSize:\n",
    "                tempy = totalSize-1\n",
    "            if m < len(model)-1:\n",
    "                modelIndex = int((tempy/totalSize)*len(model[m+1]))\n",
    "                \n",
    "        predicted_y.append(tempy)\n",
    "        \n",
    "    return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564.2778642177582\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAEKCAYAAABzHwA5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmZWSA5gZmZWSA5gZmZWSP8fnembWojaEbIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f6540a09b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# train_x=df.iloc[:,4]\n",
    "train_x=df.iloc[:,2:5]\n",
    "# train_x=df.iloc[:,0:5]\n",
    "train_y=df.iloc[:,5]\n",
    "\n",
    "start_time=time.time()\n",
    "predicted_y = predict_with_stage_model(index, train_x)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(train_y, predicted_y, edgecolors=(0, 0, 0))\n",
    "ax.plot([train_y.min(), train_y.max()], [train_y.min(), train_y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1157570\n",
      "1157570\n",
      "0.999999850544\n",
      "[ 80.08341189]\n"
     ]
    }
   ],
   "source": [
    "# estimate the result\n",
    "from sklearn.metrics import explained_variance_score\n",
    "train_y=df.iloc[:,5]\n",
    "print(len(train_y))\n",
    "print(len(predicted_y))\n",
    "\n",
    "# 1- Var(Ytrue - Ypred)/Var(Ytrue)  best: 1  the lower the worse\n",
    "result = explained_variance_score(train_y, predicted_y)\n",
    "print(result)# calculate scan item amount\n",
    "\n",
    "ind = 0\n",
    "scan = 0\n",
    "totalscan = 0\n",
    "for i in range(len(df)):\n",
    "    ind = predicted_y[i]\n",
    "    scan = abs(ind-i) # distance to exact\n",
    "    totalscan += scan\n",
    "\n",
    "print(totalscan/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
